{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# コードリーディング\n",
    "\n",
    "TensorFLowによって2値分類を行うサンプルコードを載せました。今回はこれをベースにして進めます。\n",
    "\n",
    "\n",
    "tf.kerasやtf.estimatorなどの高レベルAPIは使用していません。低レベルなところから見ていくことにします。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kei\\anaconda3\\envs\\py_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Kei\\anaconda3\\envs\\py_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Kei\\anaconda3\\envs\\py_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Kei\\anaconda3\\envs\\py_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Kei\\anaconda3\\envs\\py_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Kei\\anaconda3\\envs\\py_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Kei\\anaconda3\\envs\\py_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Kei\\anaconda3\\envs\\py_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Kei\\anaconda3\\envs\\py_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Kei\\anaconda3\\envs\\py_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Kei\\anaconda3\\envs\\py_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Kei\\anaconda3\\envs\\py_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "\n",
    "#計算グラフにノードを追加\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    #プレースホルダを定義\n",
    "    #次元の大きさが可変の時,Noneを指定\n",
    "    tf_x = tf.placeholder(shape=(None), dtype=tf.float32, name='tf_x')\n",
    "    tf_y = tf.placeholder(shape=(None), dtype=tf.float32, name='tf_y')\n",
    "    \n",
    "    #変数(モデルのパラメータ)を定義\n",
    "    weight = tf.Variable(tf.random_normal(shape=(1,1), stddev=0.25), name='weight')\n",
    "    bias = tf.Variable(0.0, name='bias')\n",
    "    \n",
    "    #モデルを構築\n",
    "    #'+'も使用できるがtf.addは結果として得られるテンソルの名前をnameパラメータに指定できる\n",
    "    y_hat = tf.add(weight*tf_x, bias, name='y_hat')\n",
    "    \n",
    "    #コストを計算\n",
    "    cost = tf.reduce_mean(tf.square(tf_y - y_hat), name='cost')\n",
    "    \n",
    "    #モデルをトレーニング\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optim.minimize(cost, name='train_op')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ランダムなデータセットを作成\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def make_random_data():\n",
    "    x = np.random.uniform(low=-2, high=4, size=200)\n",
    "    y = []\n",
    "    for t in x:\n",
    "        r = np.random.normal(loc=0.0, scale=(0.5+t*t/3), size=None)\n",
    "        y.append(r)\n",
    "    return x, 1.726 * x -0.84 + np.array(y)\n",
    "\n",
    "x, y = make_random_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08423644]]\n",
      "Epoch    0: 9.1859\n",
      "Epoch   50: 6.9381\n",
      "Epoch  100: 5.8536\n",
      "Epoch  150: 5.3063\n",
      "Epoch  200: 5.0108\n",
      "Epoch  250: 4.8362\n",
      "Epoch  300: 4.7223\n",
      "Epoch  350: 4.6408\n",
      "Epoch  400: 4.5782\n",
      "Epoch  450: 4.5278\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = x[:100], y[:100]\n",
    "x_test, y_test = x[100:], y[100:]\n",
    "\n",
    "n_epochs = 500\n",
    "\n",
    "training_cost = []\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    #計算グラフにある変数をすべて初期化するための演算子を返す\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(weight))\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        c, _ = sess.run([cost, train_op], feed_dict={tf_x: x_train, tf_y: y_train})\n",
    "        training_cost.append(c)\n",
    "        if not e % 50:\n",
    "            print('Epoch %4d: %.4f' % (e, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】スクラッチを振り返る\n",
    "ここまでのスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙してください。\n",
    "\n",
    "\n",
    "（例）\n",
    "\n",
    "- 重みを初期化する必要があった\n",
    "- エポックのループが必要だった\n",
    "\n",
    "それらがフレームワークにおいてはどのように実装されるかを今回覚えていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 重み,バイアスの初期化方法を選択\n",
    "    - SimpleInitializer(ガウス分布による初期化)\n",
    "    - Xavier\n",
    "    - HeInitializer(ReLUの時)\n",
    "    \n",
    "- 結合層の選択\n",
    "    - 全結合\n",
    "    - 畳み込み\n",
    "    \n",
    "- 活性化関数の選択\n",
    "    - Sigmoid\n",
    "    - Tanh\n",
    "    - ReLU\n",
    "    - Softmax(出力層)\n",
    "\n",
    "- 最適化手法の選択\n",
    "    - SGD(確率的勾配降下法)\n",
    "    - AdaGrad\n",
    "    \n",
    "    \n",
    "- 学習 \n",
    "    - 重み, バイアスの初期化\n",
    "    - forward\n",
    "    - 誤差の計算\n",
    "    - backward(パラメータの更新)\n",
    "    - エポックのループ\n",
    "- 推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの用意\n",
    "以前から使用しているIrisデータセットを使用します。以下のサンプルコードではIris.csvが同じ階層にある想定です。\n",
    "\n",
    "\n",
    "Iris Species\n",
    "\n",
    "\n",
    "目的変数はSpeciesですが、3種類ある中から以下の2種類のみを取り出して使用します。\n",
    "\n",
    "\n",
    "- Iris-versicolor\n",
    "- Iris-virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】スクラッチとTensorFlowの対応を考える\n",
    "以下のサンプルコードを見て、先ほど列挙した「ディープラーニングを実装するために必要なもの」がTensorFlowではどう実装されているかを確認してください。\n",
    "\n",
    "\n",
    "それを簡単に言葉でまとめてください。単純な一対一の対応であるとは限りません。\n",
    "\n",
    "\n",
    "《サンプルコード》\n",
    "\n",
    "\n",
    "＊バージョン1.5から1.14の間で動作を確認済みです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 値の設定  \n",
    "ハイパーパラメータ -> 直接値を入力  \n",
    "X, y -> tf.placeholder(データ型とshapeを指定)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "# tf.placeholderはデータが格納されている入れ物\n",
    "# データは未定のままグラフを構築し、具体的な値は実行するときに与える\n",
    "# 次元の大きさが可変の時,Noneを指定\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 計算グラフの構築  \n",
    "重みとバイアス -> tf.Valiable(学習時に更新する値)\n",
    "\n",
    "それぞれの層を定義(結合方法, 活性化関数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    #tf.Valiableは変数(モデルのパラメータ,学習時に更新する値)を定義\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    \n",
    "    #入力層,隠れ層\n",
    "    #matmulは行列積,x@w+b\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    #ReLUによる活性化\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 目的関数, 最適化手法, 指標値\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kei\\anaconda3\\envs\\py_env\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "#reduce_meanは与えたリストに入っている数値の平均値を求める関数\n",
    "#sigmoid_cross_entropy_with_logitsはシグモイド活性化関数でクロスエントロピーを取得する関数\n",
    "#二値分類なので出力を0-1に収めるシグモイド関数\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "\n",
    "# 最適化手法\n",
    "#Adam(魚本p175)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "#Adamでlossを最小化していく\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "#tf.equalは要素が一致するindexにTrueを返す\n",
    "# tf.signは要素ごとに正なら1,0なら0,負なら-1となる変換をかける\n",
    "#tf.sigmoid(logits)はy=1である確率, 50%を閾値にして-1,0,1に振り分け\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "\n",
    "# 指標値計算\n",
    "#tf.castでbool型からfloat32に変換\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 計算グラフの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, total_loss : 25.8372, val_loss : 12.5255, total_acc : 0.664, val_acc : 0.625\n",
      "Epoch 1, total_loss : 13.3242, val_loss : 0.0056, total_acc : 0.700, val_acc : 1.000\n",
      "Epoch 2, total_loss : 1.7133, val_loss : 0.0191, total_acc : 0.857, val_acc : 1.000\n",
      "Epoch 3, total_loss : 0.3963, val_loss : 0.5059, total_acc : 0.971, val_acc : 0.938\n",
      "Epoch 4, total_loss : 0.3000, val_loss : 0.0004, total_acc : 0.943, val_acc : 1.000\n",
      "Epoch 5, total_loss : 0.2845, val_loss : 0.5157, total_acc : 0.986, val_acc : 0.938\n",
      "Epoch 6, total_loss : 0.2547, val_loss : 0.0023, total_acc : 0.957, val_acc : 1.000\n",
      "Epoch 7, total_loss : 0.2589, val_loss : 0.8435, total_acc : 0.986, val_acc : 0.875\n",
      "Epoch 8, total_loss : 0.5799, val_loss : 1.5683, total_acc : 0.943, val_acc : 0.812\n",
      "Epoch 9, total_loss : 0.9330, val_loss : 5.8528, total_acc : 0.921, val_acc : 0.812\n",
      "test_acc : 0.750\n"
     ]
    }
   ],
   "source": [
    "# variableの初期化\n",
    "#計算グラフにある変数をすべて初期化するための演算子を返す\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "#Sessionで計算グラフを起動し、計算グラフの様々なノードを実行する\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        #np.ceilは小数点の切り上げ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            #値はSession.run()の引数feed_dictに辞書型で指定する\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= total_batch\n",
    "        total_acc /= total_batch\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "#         print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "        print(\"Epoch {}, total_loss : {:.4f}, val_loss : {:.4f}, total_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, total_loss, val_loss, total_acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 他のデータセットへの適用\n",
    "\n",
    "これまで扱ってきた小さなデータセットがいくつかあります。上記サンプルコードを書き換え、これらに対して学習・推定を行うニューラルネットワークを作成してください。\n",
    "\n",
    "\n",
    "- Iris（3種類全ての目的変数を使用）\n",
    "- House Prices\n",
    "\n",
    "どのデータセットもtrain, val, testの3種類に分けて使用してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】3種類全ての目的変数を使用したIrisのモデルを作成\n",
    "Irisデータセットのtrain.csvの中で、目的変数Speciesに含まれる3種類全てを分類できるモデルを作成してください。\n",
    "\n",
    "\n",
    "Iris Species\n",
    "\n",
    "\n",
    "2クラスの分類と3クラス以上の分類の違いを考慮してください。それがTensorFlowでどのように書き換えられるかを公式ドキュメントなどを参考に調べてください。\n",
    "\n",
    "\n",
    "《ヒント》\n",
    "\n",
    "\n",
    "以下の2箇所は2クラス分類特有の処理です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "メソッドは以下のように公式ドキュメントを確認してください。\n",
    "\n",
    "\n",
    "tf.nn.sigmoid_cross_entropy_with_logits  |  TensorFlow\n",
    "\n",
    "\n",
    "tf.math.sign  |  TensorFlow\n",
    "\n",
    "\n",
    "＊tf.signとtf.math.signは同じ働きをします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 変えること  \n",
    "目的変数のonehot化  \n",
    "n_class=3  \n",
    "目的関数 sigmoid -> softmax  \n",
    "推定結果tf.sign -> tf.argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "# ワンホットライブラリのインスタンス作成\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y = enc.fit_transform(y[:, np.newaxis])\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-f239075cd3d9>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数(tf.nn.sigmoid_cross_entropy_with_logitsを変更)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果(tf.sign->tf.argmax, tf.sigmoid->tf.nn.softmax)\n",
    "#tf.argmaxの第2引数を1にして行ごとの最大を返す\n",
    "correct_pred = tf.equal(tf.argmax(Y,1),tf.argmax(tf.nn.softmax(logits),1))\n",
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, total_loss : 11.1145, val_loss : 7.8915, total_acc : 0.643, val_acc : 0.708\n",
      "Epoch 1, total_loss : 2.4986, val_loss : 11.5099, total_acc : 0.793, val_acc : 0.625\n",
      "Epoch 2, total_loss : 2.0065, val_loss : 1.1870, total_acc : 0.850, val_acc : 0.917\n",
      "Epoch 3, total_loss : 0.4586, val_loss : 2.6469, total_acc : 0.940, val_acc : 0.708\n",
      "Epoch 4, total_loss : 0.8553, val_loss : 3.0087, total_acc : 0.900, val_acc : 0.750\n",
      "Epoch 5, total_loss : 1.1043, val_loss : 6.4418, total_acc : 0.900, val_acc : 0.625\n",
      "Epoch 6, total_loss : 1.7096, val_loss : 1.4422, total_acc : 0.890, val_acc : 0.833\n",
      "Epoch 7, total_loss : 0.2102, val_loss : 5.9209, total_acc : 0.970, val_acc : 0.667\n",
      "Epoch 8, total_loss : 0.6045, val_loss : 10.9283, total_acc : 0.930, val_acc : 0.625\n",
      "Epoch 9, total_loss : 2.3174, val_loss : 18.3258, total_acc : 0.843, val_acc : 0.625\n",
      "test_acc : 0.833\n"
     ]
    }
   ],
   "source": [
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        #ミニバッチの総数\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= total_batch\n",
    "        total_acc /= total_batch\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "#         print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "        print(\"Epoch {}, total_loss : {:.4f}, val_loss : {:.4f}, total_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, total_loss, val_loss, total_acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】House Pricesのモデルを作成\n",
    "回帰問題のデータセットであるHouse Pricesを使用したモデルを作成してください。\n",
    "\n",
    "\n",
    "House Prices: Advanced Regression Techniques\n",
    "\n",
    "\n",
    "この中のtrain.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使ってください。説明変数はさらに増やしても構いません。\n",
    "\n",
    "\n",
    "分類問題と回帰問題の違いを考慮してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 変えること\n",
    "\n",
    "標準化  \n",
    "学習率  \n",
    "目的変数は1列  \n",
    "目的関数を平均2乗誤差(MSE)  \n",
    "指標値計算なし  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460,)\n",
      "(1460, 1)\n"
     ]
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "dataset_path =\"train.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "y = df[\"SalePrice\"]\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "print(y.shape)\n",
    "y = y[:, np.newaxis]\n",
    "print(y.shape)\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "#標準化\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.00001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "#tf.squareで要素ごとの2乗をとり、tf.reduce_meanで平均\n",
    "loss_op = tf.reduce_mean(tf.square(Y - logits))\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, total_loss : 39323830468.0851, val_loss : 38320234496.0000\n",
      "Epoch 1, total_loss : 39323635973.4468, val_loss : 38320041984.0000\n",
      "Epoch 2, total_loss : 39323441653.1064, val_loss : 38319853568.0000\n",
      "Epoch 3, total_loss : 39323246483.0638, val_loss : 38319665152.0000\n",
      "Epoch 4, total_loss : 39323050920.8511, val_loss : 38319464448.0000\n",
      "Epoch 5, total_loss : 39322854269.2766, val_loss : 38319267840.0000\n",
      "Epoch 6, total_loss : 39322659273.5319, val_loss : 38319075328.0000\n",
      "Epoch 7, total_loss : 39322462142.6383, val_loss : 38318870528.0000\n",
      "Epoch 8, total_loss : 39322264881.0213, val_loss : 38318669824.0000\n",
      "Epoch 9, total_loss : 39322067946.2128, val_loss : 38318473216.0000\n",
      "Epoch 10, total_loss : 39321870553.8723, val_loss : 38318280704.0000\n",
      "Epoch 11, total_loss : 39321672878.2979, val_loss : 38318080000.0000\n",
      "Epoch 12, total_loss : 39321473590.4681, val_loss : 38317883392.0000\n",
      "Epoch 13, total_loss : 39321275239.4894, val_loss : 38317686784.0000\n",
      "Epoch 14, total_loss : 39321076365.6170, val_loss : 38317502464.0000\n",
      "Epoch 15, total_loss : 39320877099.5745, val_loss : 38317301760.0000\n",
      "Epoch 16, total_loss : 39320677572.0851, val_loss : 38317101056.0000\n",
      "Epoch 17, total_loss : 39320476606.6383, val_loss : 38316896256.0000\n",
      "Epoch 18, total_loss : 39320276033.3617, val_loss : 38316703744.0000\n",
      "Epoch 19, total_loss : 39320074174.6383, val_loss : 38316515328.0000\n",
      "Epoch 20, total_loss : 39319872337.7021, val_loss : 38316306432.0000\n",
      "Epoch 21, total_loss : 39319669585.7021, val_loss : 38316101632.0000\n",
      "Epoch 22, total_loss : 39319466332.5957, val_loss : 38315892736.0000\n",
      "Epoch 23, total_loss : 39319262970.5532, val_loss : 38315687936.0000\n",
      "Epoch 24, total_loss : 39319058628.0851, val_loss : 38315483136.0000\n",
      "Epoch 25, total_loss : 39318854394.5532, val_loss : 38315282432.0000\n",
      "Epoch 26, total_loss : 39318648701.2766, val_loss : 38315081728.0000\n",
      "Epoch 27, total_loss : 39318443356.5957, val_loss : 38314864640.0000\n",
      "Epoch 28, total_loss : 39318236748.2553, val_loss : 38314659840.0000\n",
      "Epoch 29, total_loss : 39318029769.5319, val_loss : 38314450944.0000\n",
      "Epoch 30, total_loss : 39317822420.4255, val_loss : 38314237952.0000\n",
      "Epoch 31, total_loss : 39317614504.8511, val_loss : 38314037248.0000\n",
      "Epoch 32, total_loss : 39317406284.2553, val_loss : 38313832448.0000\n",
      "Epoch 33, total_loss : 39317197148.5957, val_loss : 38313648128.0000\n",
      "Epoch 34, total_loss : 39316987293.9574, val_loss : 38313418752.0000\n",
      "Epoch 35, total_loss : 39316777286.8085, val_loss : 38313197568.0000\n",
      "Epoch 36, total_loss : 39316566604.2553, val_loss : 38312980480.0000\n",
      "Epoch 37, total_loss : 39316355072.0000, val_loss : 38312771584.0000\n",
      "Epoch 38, total_loss : 39316143561.5319, val_loss : 38312570880.0000\n",
      "Epoch 39, total_loss : 39315931048.8511, val_loss : 38312349696.0000\n",
      "Epoch 40, total_loss : 39315718056.8511, val_loss : 38312136704.0000\n",
      "Epoch 41, total_loss : 39315505217.3617, val_loss : 38311911424.0000\n",
      "Epoch 42, total_loss : 39315291027.0638, val_loss : 38311710720.0000\n",
      "Epoch 43, total_loss : 39315075790.9787, val_loss : 38311477248.0000\n",
      "Epoch 44, total_loss : 39314860097.3617, val_loss : 38311288832.0000\n",
      "Epoch 45, total_loss : 39314644338.3830, val_loss : 38311071744.0000\n",
      "Epoch 46, total_loss : 39314427729.7021, val_loss : 38310854656.0000\n",
      "Epoch 47, total_loss : 39314210380.2553, val_loss : 38310637568.0000\n",
      "Epoch 48, total_loss : 39313991832.5106, val_loss : 38310400000.0000\n",
      "Epoch 49, total_loss : 39313773371.9149, val_loss : 38310187008.0000\n",
      "Epoch 50, total_loss : 39313554562.7234, val_loss : 38309978112.0000\n",
      "Epoch 51, total_loss : 39313334010.5532, val_loss : 38309748736.0000\n",
      "Epoch 52, total_loss : 39313113393.0213, val_loss : 38309527552.0000\n",
      "Epoch 53, total_loss : 39312891838.6383, val_loss : 38309318656.0000\n",
      "Epoch 54, total_loss : 39312669739.5745, val_loss : 38309097472.0000\n",
      "Epoch 55, total_loss : 39312446398.6383, val_loss : 38308864000.0000\n",
      "Epoch 56, total_loss : 39312223384.5106, val_loss : 38308642816.0000\n",
      "Epoch 57, total_loss : 39311999106.7234, val_loss : 38308425728.0000\n",
      "Epoch 58, total_loss : 39311774109.9574, val_loss : 38308184064.0000\n",
      "Epoch 59, total_loss : 39311549440.0000, val_loss : 38307962880.0000\n",
      "Epoch 60, total_loss : 39311323746.0426, val_loss : 38307729408.0000\n",
      "Epoch 61, total_loss : 39311097550.9787, val_loss : 38307504128.0000\n",
      "Epoch 62, total_loss : 39310871094.4681, val_loss : 38307295232.0000\n",
      "Epoch 63, total_loss : 39310642873.1915, val_loss : 38307053568.0000\n",
      "Epoch 64, total_loss : 39310414499.4043, val_loss : 38306824192.0000\n",
      "Epoch 65, total_loss : 39310185951.3191, val_loss : 38306594816.0000\n",
      "Epoch 66, total_loss : 39309955725.6170, val_loss : 38306361344.0000\n",
      "Epoch 67, total_loss : 39309724868.0851, val_loss : 38306140160.0000\n",
      "Epoch 68, total_loss : 39309492812.2553, val_loss : 38305902592.0000\n",
      "Epoch 69, total_loss : 39309260778.2128, val_loss : 38305673216.0000\n",
      "Epoch 70, total_loss : 39309027044.7660, val_loss : 38305443840.0000\n",
      "Epoch 71, total_loss : 39308792875.5745, val_loss : 38305214464.0000\n",
      "Epoch 72, total_loss : 39308558227.0638, val_loss : 38304972800.0000\n",
      "Epoch 73, total_loss : 39308322663.4894, val_loss : 38304739328.0000\n",
      "Epoch 74, total_loss : 39308086141.2766, val_loss : 38304501760.0000\n",
      "Epoch 75, total_loss : 39307849074.3830, val_loss : 38304264192.0000\n",
      "Epoch 76, total_loss : 39307611375.6596, val_loss : 38304022528.0000\n",
      "Epoch 77, total_loss : 39307372761.8723, val_loss : 38303776768.0000\n",
      "Epoch 78, total_loss : 39307132884.4255, val_loss : 38303547392.0000\n",
      "Epoch 79, total_loss : 39306893050.5532, val_loss : 38303305728.0000\n",
      "Epoch 80, total_loss : 39306651931.2340, val_loss : 38303072256.0000\n",
      "Epoch 81, total_loss : 39306410397.9574, val_loss : 38302830592.0000\n",
      "Epoch 82, total_loss : 39306167121.7021, val_loss : 38302584832.0000\n",
      "Epoch 83, total_loss : 39305923278.9787, val_loss : 38302339072.0000\n",
      "Epoch 84, total_loss : 39305678913.3617, val_loss : 38302081024.0000\n",
      "Epoch 85, total_loss : 39305434417.0213, val_loss : 38301851648.0000\n",
      "Epoch 86, total_loss : 39305188439.1489, val_loss : 38301601792.0000\n",
      "Epoch 87, total_loss : 39304941851.2340, val_loss : 38301372416.0000\n",
      "Epoch 88, total_loss : 39304694086.8085, val_loss : 38301114368.0000\n",
      "Epoch 89, total_loss : 39304445407.3191, val_loss : 38300876800.0000\n",
      "Epoch 90, total_loss : 39304196727.8298, val_loss : 38300602368.0000\n",
      "Epoch 91, total_loss : 39303946501.4468, val_loss : 38300356608.0000\n",
      "Epoch 92, total_loss : 39303695468.9362, val_loss : 38300098560.0000\n",
      "Epoch 93, total_loss : 39303443347.0638, val_loss : 38299856896.0000\n",
      "Epoch 94, total_loss : 39303190114.0426, val_loss : 38299611136.0000\n",
      "Epoch 95, total_loss : 39302935748.0851, val_loss : 38299357184.0000\n",
      "Epoch 96, total_loss : 39302680292.7660, val_loss : 38299115520.0000\n",
      "Epoch 97, total_loss : 39302424401.7021, val_loss : 38298857472.0000\n",
      "Epoch 98, total_loss : 39302167464.8511, val_loss : 38298607616.0000\n",
      "Epoch 99, total_loss : 39301909111.8298, val_loss : 38298333184.0000\n"
     ]
    }
   ],
   "source": [
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= total_batch\n",
    "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
    "#         print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
    "        print(\"Epoch {}, total_loss : {:.4f}, val_loss : {:.4f}\".format(epoch, total_loss, val_loss))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】MNISTのモデルを作成\n",
    "\n",
    "ニューラルネットワークのスクラッチで使用したMNISTを分類するモデルを作成してください。\n",
    "\n",
    "\n",
    "3クラス以上の分類という点ではひとつ前のIrisと同様です。入力が画像であるという点で異なります。\n",
    "\n",
    "\n",
    "スクラッチで実装したモデルの再現を目指してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 変えること  \n",
    "\n",
    "平坦化  \n",
    "正規化(データを0-1の範囲に押し込める)  \n",
    "->画像は値の範囲が決まってるため(ピクセルのRGB値0-255)正規化が用いられることが多い  \n",
    "目的変数のonehot化  \n",
    "n_class=10  \n",
    "目的関数 sigmoid -> softmax  \n",
    "推定結果tf.sign -> tf.argmax  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 平坦化\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# 前処理\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "\n",
    "#正規化\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# ワンホットライブラリのインスタンス作成\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test = enc.fit_transform(y_test[:, np.newaxis])\n",
    "\n",
    "# trainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.argmax(Y,1),tf.argmax(tf.nn.softmax(logits),1))\n",
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, total_loss : 4.9291, val_loss : 1.1050, total_acc : 0.766, val_acc : 0.757\n",
      "Epoch 1, total_loss : 0.6998, val_loss : 0.6621, total_acc : 0.827, val_acc : 0.854\n",
      "Epoch 2, total_loss : 0.4213, val_loss : 0.3224, total_acc : 0.895, val_acc : 0.913\n",
      "Epoch 3, total_loss : 0.2946, val_loss : 0.3181, total_acc : 0.925, val_acc : 0.929\n",
      "Epoch 4, total_loss : 0.2575, val_loss : 0.2888, total_acc : 0.934, val_acc : 0.933\n",
      "Epoch 5, total_loss : 0.2405, val_loss : 0.2927, total_acc : 0.939, val_acc : 0.933\n",
      "Epoch 6, total_loss : 0.2378, val_loss : 0.2834, total_acc : 0.942, val_acc : 0.935\n",
      "Epoch 7, total_loss : 0.2343, val_loss : 0.2751, total_acc : 0.942, val_acc : 0.934\n",
      "Epoch 8, total_loss : 0.2253, val_loss : 0.2991, total_acc : 0.945, val_acc : 0.939\n",
      "Epoch 9, total_loss : 0.2208, val_loss : 0.2601, total_acc : 0.945, val_acc : 0.935\n",
      "test_acc : 0.938\n"
     ]
    }
   ],
   "source": [
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= total_batch\n",
    "        total_acc /= total_batch\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "#         print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "        print(\"Epoch {}, total_loss : {:.4f}, val_loss : {:.4f}, total_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, total_loss, val_loss, total_acc, val_acc))\n",
    "   \n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow入門2 ロジスティック回帰実装\n",
    "\n",
    "## ロジスティック回帰の実装\n",
    "\n",
    "TensorFlowを使いロジスティック回帰を実装していきます。入門1では単純な足し算でしたが、ここでは学習を伴う計算を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y_train = np.array([[0],[0],[0],[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "t = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([2,1]))\n",
    "b = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = tf.sigmoid(tf.matmul(x, W) + b)\n",
    "\n",
    "mat = tf.matmul(x, W)\n",
    "y = tf.sigmoid(mat + b)\n",
    "\n",
    "cross_entropy = tf.reduce_sum(-t * tf.log(y) - (1 - t) * tf.log(1 - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.sign(y - 0.5), tf.sign(t - 0.5))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Accuracy: 0.750000\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "epoch: 100, Accuracy: 1.000000\n",
      "[[0.       ]\n",
      " [1.7671354]\n",
      " [1.7671354]\n",
      " [3.5342708]]\n",
      "epoch: 200, Accuracy: 1.000000\n",
      "[[0.       ]\n",
      " [2.7020476]\n",
      " [2.7020476]\n",
      " [5.404095 ]]\n",
      "epoch: 300, Accuracy: 1.000000\n",
      "[[0.       ]\n",
      " [3.3457706]\n",
      " [3.3457706]\n",
      " [6.691541 ]]\n",
      "epoch: 400, Accuracy: 1.000000\n",
      "[[0.       ]\n",
      " [3.8412282]\n",
      " [3.8412282]\n",
      " [7.6824565]]\n",
      "epoch: 500, Accuracy: 1.000000\n",
      "[[0.       ]\n",
      " [4.2443557]\n",
      " [4.2443557]\n",
      " [8.488711 ]]\n",
      "epoch: 600, Accuracy: 1.000000\n",
      "[[0.       ]\n",
      " [4.5839767]\n",
      " [4.5839767]\n",
      " [9.1679535]]\n",
      "epoch: 700, Accuracy: 1.000000\n",
      "[[0.       ]\n",
      " [4.8771544]\n",
      " [4.8771544]\n",
      " [9.754309 ]]\n",
      "epoch: 800, Accuracy: 1.000000\n",
      "[[ 0.       ]\n",
      " [ 5.1348853]\n",
      " [ 5.1348853]\n",
      " [10.269771 ]]\n",
      "epoch: 900, Accuracy: 1.000000\n",
      "[[ 0.       ]\n",
      " [ 5.3646903]\n",
      " [ 5.3646903]\n",
      " [10.729381 ]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    sess.run(train_step, feed_dict={\n",
    "        x:x_train,\n",
    "        t:y_train\n",
    "    })\n",
    "# 100回ごとに正解率を表示\n",
    "    if epoch % 100 == 0:\n",
    "        acc_val = sess.run(\n",
    "            accuracy, feed_dict={\n",
    "                x:x_train,\n",
    "                t:y_train})\n",
    "        print ('epoch: %d, Accuracy: %f'\n",
    "               %(epoch, acc_val))\n",
    "        print(sess.run(mat, feed_dict={x:x_train,t:y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "[[1.9654632e-04]\n",
      " [4.9049824e-02]\n",
      " [4.9049824e-02]\n",
      " [9.3120384e-01]]\n"
     ]
    }
   ],
   "source": [
    "#学習結果が正しいか確認\n",
    "classified = sess.run(correct_prediction, feed_dict={\n",
    "    x:x_train,\n",
    "    t:y_train\n",
    "})\n",
    "#出力yの確認\n",
    "prob = sess.run(y, feed_dict={\n",
    "    x:x_train,\n",
    "    t:y_train\n",
    "})\n",
    "print(classified)\n",
    "# [[ True]\n",
    "# [ True]\n",
    "# [ True]\n",
    "# [ True]]\n",
    "print(prob)\n",
    "# [[  1.96514215e-04]\n",
    "# [  4.90498319e-02]\n",
    "# [  4.90498319e-02]\n",
    "# [  9.31203783e-01]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[5.5699544]\n",
      " [5.5699544]]\n",
      "b: [-8.534579]\n"
     ]
    }
   ],
   "source": [
    "print('W:', sess.run(W))\n",
    "print('b:', sess.run(b))\n",
    "# W: [[ 5.5699544]\n",
    "# [ 5.5699544]]\n",
    "# b: [-8.53457928]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
